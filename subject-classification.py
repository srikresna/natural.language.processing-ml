# -*- coding: utf-8 -*-
"""subject-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OPpmVIXnlOJMU-ZX31dESkGYHDA7Utc9

<h1>QUESTION SUBJECT DETECTION</h1>
The model in this project can read and detect a sentence that has been input. This model will determine which sentence is part of what subject. This model will be useful for use by librarians in sorting books based on just a piece of content. Or useful for other people related to education.

<h2> ABOUT DATASET </h5>
<h5>IITJEE NEET AIIMS Students Questions Data</h5>
Subject classification from Questions

* https://www.kaggle.com/datasets/mrutyunjaybiswal/iitjee-neet-aims-students-questions-data

In India, every year lacs of students sit for competitive examinations like JEE Advanced, JEE Mains, NEET, etc. These exams are said to be the gateway to get admission into India's premier Institutes such as IITs, NITs, AIIMS, etc. Keeping in mind that the competition is tough as lacs of students appear for these examinations, there has been an enormous development in Ed Tech Industry in India, fortuning the dreams of lacs of aspirants via providing online as well as offline coaching, mentoring, etc. This particular dataset consists of questions/doubts raised by students preparing for such examinations.

Content
The dataset contains Students-questions.csv file in version 1 as of now.
Inside the CSV file, we have two columns:

* eng: The full question or description of the questions
* Subject: Which subject does the question belong to. It has 4 classes, Physics, Chemistry, Biology, and Mathematics.

<h1> WHAT ARE YOU WAITING FOR? LET'S GO!!

Donwload Dataset from Kaggle
"""

!pip install -q kaggle

from google.colab import files
files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d mrutyunjaybiswal/iitjee-neet-aims-students-questions-data

!unzip iitjee-neet-aims-students-questions-data.zip

"""Import Library"""

import pandas as pd
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.model_selection import train_test_split

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from nltk.stem import WordNetLemmatizer 
from nltk.corpus import stopwords
stopwords.words('english')
import string
string.punctuation
from nltk.stem.porter import PorterStemmer

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping

"""Understanding Dataset"""

df = pd.read_csv('subjects-questions.csv')
df

df.Subject.unique()

df.isna().sum()

"""Exploratory Data Analysis (EDA)"""

df.head()

plt.figure(figsize = (8, 8))
sns.countplot(df['Subject'])

"""From the countplot above, biology have a less example then another

Data Pre-Processing

because this dataset is categorical, we need doing one-hot encoding first
"""

category = pd.get_dummies(df.Subject)
df_new = pd.concat([df, category], axis = 1)
df_new = df_new.drop(columns = 'Subject')

df_new

"""Text Processing

Make functions for removal of stopwords, lemmatizing and cleaning text:
"""

def remove_Stopwords(text ):
    stop_words = set(stopwords.words('english')) 
    words = word_tokenize( text.lower() ) 
    sentence = [w for w in words if not w in stop_words]
    return " ".join(sentence)
    

def lemmatize_text(text):
    wordlist=[]
    lemmatizer = WordNetLemmatizer() 
    sentences=sent_tokenize(text)
    for sentence in sentences:
        words=word_tokenize(sentence)
        for word in words:
            wordlist.append(lemmatizer.lemmatize(word))
    return ' '.join(wordlist) 

def clean_text(text ): 
    delete_dict = {sp_character: '' for sp_character in string.punctuation} 
    delete_dict[' '] = ' ' 
    table = str.maketrans(delete_dict)
    text1 = text.translate(table)
    textArr= text1.split()
    text2 = ' '.join([w for w in textArr]) 
    
    return text2.lower()

def stemSentence(text):
    porter = PorterStemmer()
    token_words=word_tokenize(text)
    token_words
    stem_sentence=[]
    for word in token_words:
        stem_sentence.append(porter.stem(word))
        stem_sentence.append(" ")
    return "".join(stem_sentence)

df_new['new_eng'] = df_new['eng'].apply(remove_Stopwords)
df_new['new_eng'] = df_new['eng'].apply(lemmatize_text)
df_new['new_eng'] = df_new['eng'].apply(clean_text)
df_new

df_new['stem_eng'] = df_new['new_eng'].apply(stemSentence)

df_new

"""<h1>Model Building"""

length = df_new['stem_eng'].str.len().max()
df_new.columns

"""Change into numpy array"""

news = df_new['stem_eng'].values
label = df_new[['Biology', 'Chemistry', 'Maths', 'Physics']].values
label, news

"""Divided into training and test"""

news_train, news_test, label_train, label_test = train_test_split(news, label, test_size = 0.2, random_state = 123)

"""Tokenizer dataset"""

tokenizer = Tokenizer(num_words = length, oov_token = '<OOV>')
tokenizer.fit_on_texts(news_train)
tokenizer.fit_on_texts(news_test)

sequences_train = tokenizer.texts_to_sequences(news_train)
sequences_test = tokenizer.texts_to_sequences(news_test)

padded_train = pad_sequences(sequences_train,
                             maxlen = 5,
                             padding = 'post',
                             truncating = 'post')
padded_test = pad_sequences(sequences_test,
                            maxlen = 5,
                            padding = 'post',
                            truncating = 'post')

padded_train

"""use sequential model"""

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=10000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(254, activation='relu'),
    tf.keras.layers.Dense(128, activation= 'relu'),
    tf.keras.layers.Dense(4, activation='softmax')
])

#compile

model.compile(loss ='categorical_crossentropy',
              optimizer = 'adam',
              metrics = ['accuracy'])

#cek model

model.summary()

#Callback Function
class accCallback(Callback):
   def on_epoch_end(self, epoch, logs={}):
        if(logs.get('accuracy') >= 0.98 and logs.get('val_accuracy') >= 0.98):
            print("\nAccuracy and Val_Accuracy has reached 90%!", "\nEpoch: ", epoch)
            self.model.stop_training = True

callbacks = accCallback()

auto_reduction_LR = ReduceLROnPlateau(
    monitor = 'val_accuracy',
    patience = 2, #setelah 2 epoch, jika tidak ada kenaikan maka LR berkurang
    verbose = 1,
    factor = 0.2,
    min_lr = 0.000003
)

auto_stop_learn = EarlyStopping(
    monitor = 'val_accuracy',
    min_delta = 0,
    patience = 4,
    verbose = 1,
    mode = 'auto' 
)

#latih model
history = model.fit(padded_train, label_train,
                    steps_per_epoch = 30,
                    epochs = 100,
                    validation_data = (padded_test, label_test),
                    verbose = 1,
                    validation_steps = 50,
                    callbacks=[callbacks, auto_reduction_LR, auto_stop_learn], 
                    )

#plotting

pd.DataFrame(history.history).plot(figsize=(7, 4))
plt.grid(True)
plt.gca().set_ylim(0,3) #sumbu y

plt.show()

"""Because accuracy not increase significantly and stop in 81%, we can conclude this model is OK but still need improvement. """